{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxW9uqn6liV5l06ImPuOp5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janardhana-vels/CSE_ECE_C/blob/master/NLP_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71O-bCqiqkY7",
        "colab_type": "code",
        "outputId": "edcb5d56-6c23-46f8-cb93-5c598eda8322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install nltk\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKgeY46uqn4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MovSyJ_eq5s2",
        "colab_type": "code",
        "outputId": "46b1a2c4-2d0e-497b-bacd-371a56b6a20b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jTifafKrZSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "import nltk.corpus\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4RplABgxHDv",
        "colab_type": "code",
        "outputId": "d883529d-f626-409e-b58c-d8efd97a152f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        " nltk.download('gutenberg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epa2ip9BvaxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hh=nltk.corpus.gutenberg.words(\"shakespeare-hamlet.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko6FOX8AyIci",
        "colab_type": "code",
        "outputId": "ffdb2ce9-8ae4-48de-c7ab-e4e146fd98b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(hh)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iMO0gcdyQuh",
        "colab_type": "code",
        "outputId": "7464adac-abe7-4ef0-a49c-f570ecb6fac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "for word in hh[:500]:\n",
        "  print(word,sep='  ',end='  ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  The  Tragedie  of  Hamlet  by  William  Shakespeare  1599  ]  Actus  Primus  .  Scoena  Prima  .  Enter  Barnardo  and  Francisco  two  Centinels  .  Barnardo  .  Who  '  s  there  ?  Fran  .  Nay  answer  me  :  Stand  &  vnfold  your  selfe  Bar  .  Long  liue  the  King  Fran  .  Barnardo  ?  Bar  .  He  Fran  .  You  come  most  carefully  vpon  your  houre  Bar  .  '  Tis  now  strook  twelue  ,  get  thee  to  bed  Francisco  Fran  .  For  this  releefe  much  thankes  :  '  Tis  bitter  cold  ,  And  I  am  sicke  at  heart  Barn  .  Haue  you  had  quiet  Guard  ?  Fran  .  Not  a  Mouse  stirring  Barn  .  Well  ,  goodnight  .  If  you  do  meet  Horatio  and  Marcellus  ,  the  Riuals  of  my  Watch  ,  bid  them  make  hast  .  Enter  Horatio  and  Marcellus  .  Fran  .  I  thinke  I  heare  them  .  Stand  :  who  '  s  there  ?  Hor  .  Friends  to  this  ground  Mar  .  And  Leige  -  men  to  the  Dane  Fran  .  Giue  you  good  night  Mar  .  O  farwel  honest  Soldier  ,  who  hath  relieu  '  d  you  ?  Fra  .  Barnardo  ha  '  s  my  place  :  giue  you  goodnight  .  Exit  Fran  .  Mar  .  Holla  Barnardo  Bar  .  Say  ,  what  is  Horatio  there  ?  Hor  .  A  peece  of  him  Bar  .  Welcome  Horatio  ,  welcome  good  Marcellus  Mar  .  What  ,  ha  '  s  this  thing  appear  '  d  againe  to  night  Bar  .  I  haue  seene  nothing  Mar  .  Horatio  saies  ,  '  tis  but  our  Fantasie  ,  And  will  not  let  beleefe  take  hold  of  him  Touching  this  dreaded  sight  ,  twice  seene  of  vs  ,  Therefore  I  haue  intreated  him  along  With  vs  ,  to  watch  the  minutes  of  this  Night  ,  That  if  againe  this  Apparition  come  ,  He  may  approue  our  eyes  ,  and  speake  to  it  Hor  .  Tush  ,  tush  ,  '  twill  not  appeare  Bar  .  Sit  downe  a  -  while  ,  And  let  vs  once  againe  assaile  your  eares  ,  That  are  so  fortified  against  our  Story  ,  What  we  two  Nights  haue  seene  Hor  .  Well  ,  sit  we  downe  ,  And  let  vs  heare  Barnardo  speake  of  this  Barn  .  Last  night  of  all  ,  When  yond  same  Starre  that  '  s  Westward  from  the  Pole  Had  made  his  course  t  '  illume  that  part  of  Heauen  Where  now  it  burnes  ,  Marcellus  and  my  selfe  ,  The  Bell  then  beating  one  Mar  .  Peace  ,  breake  thee  of  :  Enter  the  Ghost  .  Looke  where  it  comes  againe  Barn  .  In  the  same  figure  ,  like  the  King  that  '  s  dead  Mar  .  Thou  art  a  Scholler  ;  speake  to  it  Horatio  Barn  .  Lookes  it  not  like  the  King  ?  Marke  it  Horatio  Hora  .  Most  like  :  It  harrowes  me  with  fear  &  wonder  Barn  .  It  would  be  spoke  too  Mar  .  Question  it  Horatio  Hor  .  What  art  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NNa8s49yflx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AI = \"\"\"According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”.\n",
        "\n",
        "Artificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think.\n",
        "\n",
        "AI is accomplished by studying how human brain thinks,2000 and how humans learn, decide, and work while trying to solve a problem, 3000 and then using the outcomes of this study as a basis of developing intelligent software and systems.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq4EyZMwzWat",
        "colab_type": "code",
        "outputId": "6274862a-bf3f-4393-f213-4d0f73fb5b0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(AI)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwxoZtu_zY9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize  import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N77fywOtz7t6",
        "colab_type": "code",
        "outputId": "c25119a9-6485-49d3-be0c-61385b57e202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNB44ninztTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AI_Tokens   =   word_tokenize(AI)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJt4tghxz-aE",
        "colab_type": "code",
        "outputId": "a8696b31-25e8-48be-e10b-b06b7cc12d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(AI_Tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['According', 'to', 'the', 'father', 'of', 'Artificial', 'Intelligence', ',', 'John', 'McCarthy', ',', 'it', 'is', '“', 'The', 'science', 'and', 'engineering', 'of', 'making', 'intelligent', 'machines', ',', 'especially', 'intelligent', 'computer', 'programs', '”', '.', 'Artificial', 'Intelligence', 'is', 'a', 'way', 'of', 'making', 'a', 'computer', ',', 'a', 'computer-controlled', 'robot', ',', 'or', 'a', 'software', 'think', 'intelligently', ',', 'in', 'the', 'similar', 'manner', 'the', 'intelligent', 'humans', 'think', '.', 'AI', 'is', 'accomplished', 'by', 'studying', 'how', 'human', 'brain', 'thinks', ',', 'and', 'how', 'humans', 'learn', ',', 'decide', ',', 'and', 'work', 'while', 'trying', 'to', 'solve', 'a', 'problem', ',', 'and', 'then', 'using', 'the', 'outcomes', 'of', 'this', 'study', 'as', 'a', 'basis', 'of', 'developing', 'intelligent', 'software', 'and', 'systems', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sA4vUSD0Dfy",
        "colab_type": "code",
        "outputId": "332ffdcd-9b5b-4246-ed0f-6b86a528b8e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(AI_Tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "102"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4PuRqQg0Ijt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.probability import  FreqDist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoG8p69J0brM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fdist = FreqDist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybxpK4Ol0ufX",
        "colab_type": "code",
        "outputId": "08ac136c-3139-4f45-b43f-0cb0310d615b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for word in AI_Tokens:\n",
        "    fdist[word.lower()]+=1\n",
        "fdist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({',': 10,\n",
              "          '.': 3,\n",
              "          'a': 6,\n",
              "          'accomplished': 1,\n",
              "          'according': 1,\n",
              "          'ai': 1,\n",
              "          'and': 5,\n",
              "          'artificial': 2,\n",
              "          'as': 1,\n",
              "          'basis': 1,\n",
              "          'brain': 1,\n",
              "          'by': 1,\n",
              "          'computer': 2,\n",
              "          'computer-controlled': 1,\n",
              "          'decide': 1,\n",
              "          'developing': 1,\n",
              "          'engineering': 1,\n",
              "          'especially': 1,\n",
              "          'father': 1,\n",
              "          'how': 2,\n",
              "          'human': 1,\n",
              "          'humans': 2,\n",
              "          'in': 1,\n",
              "          'intelligence': 2,\n",
              "          'intelligent': 4,\n",
              "          'intelligently': 1,\n",
              "          'is': 3,\n",
              "          'it': 1,\n",
              "          'john': 1,\n",
              "          'learn': 1,\n",
              "          'machines': 1,\n",
              "          'making': 2,\n",
              "          'manner': 1,\n",
              "          'mccarthy': 1,\n",
              "          'of': 5,\n",
              "          'or': 1,\n",
              "          'outcomes': 1,\n",
              "          'problem': 1,\n",
              "          'programs': 1,\n",
              "          'robot': 1,\n",
              "          'science': 1,\n",
              "          'similar': 1,\n",
              "          'software': 2,\n",
              "          'solve': 1,\n",
              "          'study': 1,\n",
              "          'studying': 1,\n",
              "          'systems': 1,\n",
              "          'the': 5,\n",
              "          'then': 1,\n",
              "          'think': 2,\n",
              "          'thinks': 1,\n",
              "          'this': 1,\n",
              "          'to': 2,\n",
              "          'trying': 1,\n",
              "          'using': 1,\n",
              "          'way': 1,\n",
              "          'while': 1,\n",
              "          'work': 1,\n",
              "          '“': 1,\n",
              "          '”': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPKChCeGpZqi",
        "colab_type": "code",
        "outputId": "07f44bc8-077e-4d7b-fd76-b6ffffae9b58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fdist['while']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbWRNVuNqR5Y",
        "colab_type": "code",
        "outputId": "332e5cdd-69b0-4d3b-9c27-a96d7daa3716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(fdist)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf8ZfRRp1Cwt",
        "colab_type": "code",
        "outputId": "eb71ea5a-2032-45e2-df0d-a5c98ab26b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import blankline_tokenize\n",
        "AI_Blank = blankline_tokenize(AI)\n",
        "len(AI_Blank)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS8AXiZ_1ikF",
        "colab_type": "code",
        "outputId": "4a5d457e-fc99-4964-9d21-cadce3772101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "fdist_top10 = fdist.most_common(10)\n",
        "fdist_top10"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 10),\n",
              " ('a', 6),\n",
              " ('the', 5),\n",
              " ('of', 5),\n",
              " ('and', 5),\n",
              " ('intelligent', 4),\n",
              " ('is', 3),\n",
              " ('.', 3),\n",
              " ('to', 2),\n",
              " ('artificial', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6P_R3RqsoDS",
        "colab_type": "text"
      },
      "source": [
        "***Regular Expression ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-rOUMZKpMrM",
        "colab_type": "code",
        "outputId": "a217d3c3-a665-4374-d7a0-37d764b86eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "regexp_tokenize(AI,pattern=\"\\d+\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2000', '3000']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa6K8-qdtXzP",
        "colab_type": "text"
      },
      "source": [
        "***Blank Line Tokenize***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anEPlv99szMv",
        "colab_type": "code",
        "outputId": "d04ad754-f73f-4e21-ac6b-448e6d489911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import blankline_tokenize\n",
        "AI_BlankLine = blankline_tokenize(AI)\n",
        "len(AI_BlankLine)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY2k8LjKuQKS",
        "colab_type": "text"
      },
      "source": [
        "***Sentence Tokenize***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D71vwGVItl4Y",
        "colab_type": "code",
        "outputId": "9843eef6-e108-4c0a-b204-24afe3f45c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "Sent_Token = sent_tokenize(AI)\n",
        "Sent_Token"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['According to the father of Artificial Intelligence, John McCarthy, it is “The science and engineering of making intelligent machines, especially intelligent computer programs”.',\n",
              " 'Artificial Intelligence is a way of making a computer, a computer-controlled robot, or a software think intelligently, in the similar manner the intelligent humans think.',\n",
              " 'AI is accomplished by studying how human brain thinks,2000 and how humans learn, decide, and work while trying to solve a problem, 3000 and then using the outcomes of this study as a basis of developing intelligent software and systems.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-6sH96awPFL",
        "colab_type": "text"
      },
      "source": [
        "***Bigrams,Trigrams & NGrams***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiOaKEgvwOMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.util import bigrams,trigrams,ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfCVAFeFwgSX",
        "colab_type": "code",
        "outputId": "ac608129-14de-4155-d986-01286bcf9748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "string = \"NLP is divided into two  parts NLU and NLG. Understanding is harder than Generations\"\n",
        "s_tokens = nltk.word_tokenize(string)\n",
        "s_tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NLP',\n",
              " 'is',\n",
              " 'divided',\n",
              " 'into',\n",
              " 'two',\n",
              " 'parts',\n",
              " 'NLU',\n",
              " 'and',\n",
              " 'NLG',\n",
              " '.',\n",
              " 'Understanding',\n",
              " 'is',\n",
              " 'harder',\n",
              " 'than',\n",
              " 'Generations']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cg89BwXxAR-",
        "colab_type": "code",
        "outputId": "d5362634-341c-4660-edd1-2c856c080245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "s_bigrams = list(nltk.bigrams(s_tokens))\n",
        "\n",
        "s_bigrams"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NLP', 'is'),\n",
              " ('is', 'divided'),\n",
              " ('divided', 'into'),\n",
              " ('into', 'two'),\n",
              " ('two', 'parts'),\n",
              " ('parts', 'NLU'),\n",
              " ('NLU', 'and'),\n",
              " ('and', 'NLG'),\n",
              " ('NLG', '.'),\n",
              " ('.', 'Understanding'),\n",
              " ('Understanding', 'is'),\n",
              " ('is', 'harder'),\n",
              " ('harder', 'than'),\n",
              " ('than', 'Generations')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGoisznOxhaI",
        "colab_type": "code",
        "outputId": "2414b10f-98ff-464d-de5f-f943bdff9660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "s_trigrams = list(nltk.trigrams(s_tokens))\n",
        "\n",
        "s_trigrams"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NLP', 'is', 'divided'),\n",
              " ('is', 'divided', 'into'),\n",
              " ('divided', 'into', 'two'),\n",
              " ('into', 'two', 'parts'),\n",
              " ('two', 'parts', 'NLU'),\n",
              " ('parts', 'NLU', 'and'),\n",
              " ('NLU', 'and', 'NLG'),\n",
              " ('and', 'NLG', '.'),\n",
              " ('NLG', '.', 'Understanding'),\n",
              " ('.', 'Understanding', 'is'),\n",
              " ('Understanding', 'is', 'harder'),\n",
              " ('is', 'harder', 'than'),\n",
              " ('harder', 'than', 'Generations')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQG-WJpwxqfm",
        "colab_type": "code",
        "outputId": "431e95ce-0b28-4b14-8e47-f013711e3c55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "n_grams = list(nltk.ngrams(s_tokens , 5))\n",
        "n_grams"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('NLP', 'is', 'divided', 'into', 'two'),\n",
              " ('is', 'divided', 'into', 'two', 'parts'),\n",
              " ('divided', 'into', 'two', 'parts', 'NLU'),\n",
              " ('into', 'two', 'parts', 'NLU', 'and'),\n",
              " ('two', 'parts', 'NLU', 'and', 'NLG'),\n",
              " ('parts', 'NLU', 'and', 'NLG', '.'),\n",
              " ('NLU', 'and', 'NLG', '.', 'Understanding'),\n",
              " ('and', 'NLG', '.', 'Understanding', 'is'),\n",
              " ('NLG', '.', 'Understanding', 'is', 'harder'),\n",
              " ('.', 'Understanding', 'is', 'harder', 'than'),\n",
              " ('Understanding', 'is', 'harder', 'than', 'Generations')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJTSsu9b4ctv",
        "colab_type": "text"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjZHyh1Q4cBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwSXYxpqXFbB",
        "colab_type": "code",
        "outputId": "8cb04a79-e3e4-4906-b0ed-34a4c60e0788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pst.stem(\"sleeping\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sleep'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NtrkwIUXdpQ",
        "colab_type": "code",
        "outputId": "533a4c8f-d03b-4e3f-a268-98955fdcc565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "words_stem = [\"have\" , \"having\" ,\"had\"]\n",
        "for words in words_stem:\n",
        "  print(words+\"  \"+pst.stem(words))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "have  have\n",
            "having  have\n",
            "had  had\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neiydlvgaXaV",
        "colab_type": "code",
        "outputId": "8f06e520-6f9c-4cf9-8b63-bd06c5ff5f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lst = LancasterStemmer()\n",
        "words_stem = [\"have\" , \"having\" ,\"had\"]\n",
        "for words in words_stem:\n",
        "  print(words+\"  \"+lst.stem(words))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "have  hav\n",
            "having  hav\n",
            "had  had\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JetkTucwbi4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "sbt = SnowballStemmer('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLxUApRPGEow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d2ff9f43-d6d5-4a73-f953-d114c7ff43ee"
      },
      "source": [
        "words_stem = [\"have\" , \"having\" ,\"had\"]\n",
        "for words in words_stem:\n",
        "  print(words+\"  \"+sbt.stem(words))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "have  have\n",
            "having  have\n",
            "had  had\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic1IDNLJG-FN",
        "colab_type": "text"
      },
      "source": [
        "***Lemmatization***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEcXibX6HsUh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "11f31467-47e3-4abb-8646-ab2ca7b6808d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPI5nb-3G9QU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "word_lem = WordNetLemmatizer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5vcM3afHe_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "149d2841-aff2-45c2-eeb1-e49ee07fdedf"
      },
      "source": [
        "word_lem.lemmatize(\"geese\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goose'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHMEGiIJH4z1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "48e82b49-11ef-48af-db99-ad655f5d84ae"
      },
      "source": [
        "for word in words_stem:\n",
        "  print(word+\"  \"+word_lem.lemmatize(word))\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "have  have\n",
            "having  having\n",
            "had  had\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gKJGd6zINQU",
        "colab_type": "text"
      },
      "source": [
        "***Stop Words***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bizQIwvaIqUF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "d07bad72-9b05-46a1-df4b-d134661605af"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHA2t3pZILXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODMKLDq_IaLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sw = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrp-z1xQIwf6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5636c2e2-9c83-40f3-e6b9-6367c97998d4"
      },
      "source": [
        "sw"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiCx7YJKIzhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ecd13383-fcf2-413c-c33d-dcb188863a52"
      },
      "source": [
        "len(sw)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hWhr_j3J7RQ",
        "colab_type": "text"
      },
      "source": [
        "***POS***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuHbAF5tJ-IZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "63e4638d-3e2b-422f-83d2-c4b57b310b7e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix-IJU-JI1xL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent = \"James Bond is one of the best characters in HollyWood\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuLoKS_UJa7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize  import word_tokenize\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNB1uk7OJyqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_tokens = word_tokenize(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcHR3mq-J24Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "e32812f8-008a-4dab-884e-1e6dc95980d1"
      },
      "source": [
        "for token in sent_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('James', 'NNP')]\n",
            "[('Bond', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('one', 'CD')]\n",
            "[('of', 'IN')]\n",
            "[('the', 'DT')]\n",
            "[('best', 'JJS')]\n",
            "[('characters', 'NNS')]\n",
            "[('in', 'IN')]\n",
            "[('HollyWood', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsAYiQrrKmwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent2 = \"John is eating the delicious Sweet\"\n",
        "sent_tokens2 = word_tokenize(sent2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tqfYHSCK4cE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "0ee01538-7d0a-451e-8f1b-3a034bd7cab7"
      },
      "source": [
        "for token in sent_tokens2:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('John', 'NNP')]\n",
            "[('is', 'VBZ')]\n",
            "[('eating', 'VBG')]\n",
            "[('the', 'DT')]\n",
            "[('delicious', 'JJ')]\n",
            "[('Sweet', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY2blKJVLtni",
        "colab_type": "text"
      },
      "source": [
        "***Name Entity Recognition***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76-jEW3lMuH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "bc381fca-06ea-4dca-9db1-c07db4453339"
      },
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozvBWQBnLtWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NE_SENT = \"The US PRESIDENT STAYS IN WASHINGTON\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1-O-jLOMIfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NE_TOKENS = word_tokenize(NE_SENT)\n",
        "NE_TAGS = nltk.pos_tag(NE_TOKENS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB04OT8BMYFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import ne_chunk\n",
        "NE_NER = ne_chunk(NE_TAGS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbobXDrANBtf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "ef258b32-1f19-4cef-f63d-4ffe464e6e2a"
      },
      "source": [
        "print(NE_NER)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (ORGANIZATION US/NNP)\n",
            "  PRESIDENT/NNP\n",
            "  STAYS/NNP\n",
            "  IN/NNP\n",
            "  (GPE WASHINGTON/NNP))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mrz_rFhlNjeT",
        "colab_type": "text"
      },
      "source": [
        "***Chunking***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pae2WRQ-NO9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "d5317dcd-37b5-41d9-f03b-87d2efff239a"
      },
      "source": [
        "new = \"The Quick brown fox  jumps over the lazy dogs\"\n",
        "new_tokens = nltk.pos_tag(word_tokenize(new))\n",
        "new_tokens"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('Quick', 'NNP'),\n",
              " ('brown', 'NN'),\n",
              " ('fox', 'NN'),\n",
              " ('jumps', 'VBZ'),\n",
              " ('over', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('lazy', 'JJ'),\n",
              " ('dogs', 'NNS')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeDePzwSN-IT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grammer_np = r\"NP:{<DT>?<JJ>*<NN>}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHUCy5ABOXy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chunk_parser = nltk.RegexpParser(grammer_np)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-bb-0wMOkvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "729eb391-822c-47b7-dcd0-b4e9f28b2d58"
      },
      "source": [
        "chunk_result = chunk_parser.parse(new_tokens)\n",
        "print(chunk_result)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  Quick/NNP\n",
            "  (NP brown/NN)\n",
            "  (NP fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  the/DT\n",
            "  lazy/JJ\n",
            "  dogs/NNS)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}